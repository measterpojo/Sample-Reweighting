{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3XH19LtLT0tc7dOviMc6P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Introduction**"],"metadata":{"id":"UyjCh7Wqc_Uy"}},{"cell_type":"markdown","source":["Sample reweighting is a common technique in domain adaptation, where the goal is to handle distribution shifts between a source domain (used for training) and a target domain (used for deployment). The idea is to assign weights to source domain samples to make them more representative of the target domain. This way, the model can better adapt to the target domain's characteristics."],"metadata":{"id":"Nbx9GLdbc-N1"}},{"cell_type":"markdown","source":["**Imports**"],"metadata":{"id":"cHgT4Ter4zTz"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","\n","from torchvision import datasets, transforms\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","import kagglehub"],"metadata":{"id":"Y7v9DKIPdDSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"MxtSu_d9A386"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"bistaumanga/usps-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zT0TmLH76NAT","executionInfo":{"status":"ok","timestamp":1743376745614,"user_tz":300,"elapsed":1026,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"1d32fe51-f030-4503-f68b-187f4abac567"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/bistaumanga/usps-dataset/versions/1\n"]}]},{"cell_type":"code","source":["# Transform for normalization\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                 transforms.Normalize((0.5,), (0.5,))])\n","\n","# Download and load MNIST dataset\n","mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8gbqNEqWZ69","executionInfo":{"status":"ok","timestamp":1743376750103,"user_tz":300,"elapsed":1849,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"b703b138-e4a2-4027-b015-ecc230044e3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 53.0MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.71MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 14.6MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 5.04MB/s]\n"]}]},{"cell_type":"markdown","source":["**Data Processing**"],"metadata":{"id":"eWSt80on412Y"}},{"cell_type":"markdown","source":["Set Up Sample Weights for Reweighting: You could calculate weights using a domain classifier, Kernel Density Estimation, or other metrics to align MNIST samples closer to USPS.\n","\n"],"metadata":{"id":"8bCmHES85FbC"}},{"cell_type":"code","source":["# Example: Random weights for MNIST samples\n","mnist_weights = torch.rand(len(mnist_train))\n","\n","# Normalize weights\n","mnist_weights /= mnist_weights.sum()"],"metadata":{"id":"2rIb0nrW4581"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mnist_weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8hRsH8O7Tdh","executionInfo":{"status":"ok","timestamp":1743386399489,"user_tz":300,"elapsed":73,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"7c248f9b-f8b9-4095-9f2e-f2c3c83610ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([3.5041e-06, 3.2884e-05, 2.3945e-05,  ..., 3.8541e-06, 4.4937e-06,\n","        2.8864e-05])"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["sampler = WeightedRandomSampler(weights=mnist_weights, num_samples=len(mnist_weights), replacement=True)\n","mnist_loader = DataLoader(mnist_train, sampler=sampler, batch_size=64)"],"metadata":{"id":"pBS8ny4D-dlA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Model**"],"metadata":{"id":"FT-_VxSGADLj"}},{"cell_type":"code","source":["# Dummy model (binary classification for simplicity)\n","class SimpleModel(nn.Module):\n","    def __init__(self):\n","        super(SimpleModel, self).__init__()\n","        self.fc = nn.Linear(28 * 28, 1)  # Input size for flattened images (28x28)\n","\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # Flatten\n","        return self.fc(x)\n","\n","\n","model = SimpleModel().to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"o9luBENsAJcZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train Loop**"],"metadata":{"id":"tVXNOC4qNmSl"}},{"cell_type":"code","source":["# Training Loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    model.train()\n","\n","    total_loss = 0.0\n","    for mnist_batch in mnist_loader:  # Training on MNIST (source domain)\n","        inputs, labels = mnist_batch\n","        inputs, labels = inputs.to(device), labels.float().to(device)\n","\n","        # Example sample weights (random for now, replace with real weights)\n","        sample_weights = torch.rand(len(labels)).to(device)\n","\n","        # Forward pass\n","        logits = model(inputs)\n","        loss = criterion(logits.squeeze(), labels)\n","\n","        # Apply sample weights\n","        weighted_loss = (loss * sample_weights).mean()\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        weighted_loss.backward()\n","        optimizer.step()\n","\n","        total_loss += weighted_loss.item()\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n","\n","    # Evaluate on USPS (target domain)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for usps_batch in usps_loader:\n","            inputs, labels = usps_batch\n","            inputs, labels = inputs.to(device), labels.float().to(device)\n","\n","            logits = model(inputs)\n","            predictions = (torch.sigmoid(logits.squeeze()) > 0.5).float()\n","            correct += (predictions == labels).sum().item()\n","            total += len(labels)\n","\n","    accuracy = correct / total * 100\n","    print(f\"Evaluation Accuracy on USPS: {accuracy:.2f}%\")\n"],"metadata":{"id":"HHauz11z_WKJ"},"execution_count":null,"outputs":[]}]}